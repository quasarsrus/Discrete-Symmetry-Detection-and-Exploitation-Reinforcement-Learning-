using JLD2
using HDF5
using Plots
using Tables

# Initialise state space

function initialise()
    s1 = Vector{Float64}()
    s2 = Vector{Float64}()
    s1 = range(-20,stop=20, length = 41);
    s2 = range(-20,stop=20, length = 41);
    sx = hcat(s1,s2)

    actionspace = ["U", "D", "R", "L", "N"]
    
    return s1, s2, sx, actionspace
end

# Initialise action space

function actionval(actionspace)
    actionspace1 = Dict()
    for i in 1:length(actionspace)
        if actionspace[i] == "U" || actionspace[i] == "D"
            actionspace1[actionspace[i]] = (0,-1*(-1)^i)
        elseif  actionspace[i] == "N"
            actionspace1[actionspace[i]] = (0,0)               
        else
            actionspace1[actionspace[i]] = (-1*(-1)^i,0)
        end
    end
    return actionspace1
end

# Define the dynamics and reward function

function move(action, state)
    
    reward = 0
    
    statex = state[1] + action[1]
    statey = state[2] + action[2]
    if (statex,statey) == (0,0)
        reward = 100
        done = true
        return (statex,statey), reward, done
    
    elseif statex > 20.0 || statex < -20.0 || statey > 20.0 || statey < -20.0
        reward = -50
        done = false
        return (state[1],state[2]), reward, done
    
    else
        reward = -10
        done = false
        return (statex,statey), reward, done  
    end
end

# Reset the states

function reset1(sx)
    return (sx[rand(1:length(sx[:,1])),1],sx[rand(1:length(sx[:,2])),2]), false
end

# Choose random action
            
function randomaction(actionspace)
    return actionspace[rand(1:length(actionspace))]
end

# Choose the action with the maximum Q-value for a state
                    
function maxaction(state, Q, actionspace)
    actval = [Q[state,i] for i in actionspace]
    actind = argmax(actval)
    return actionspace[actind]
end

# Initialise a Q-table
    
function qtable(s1, s2, actionspace)
    
    Q = Dict()
    states= [] 
    for i in s1
        for j in s2
            push!(states,(i,j))
        end
    end
    
    for i in states
        for j in actionspace
            Q[i,j] = 0 #(Int(rand(Int8)))^2
        end
    end
    return Q
end

# Main function (no symmetry detection or exploitation)

function main()
    alph = 0.3
    gamm = 0.9
    episodes = 10
    
    iter = 300
    
    totalr = []
    score_cumulative = []
    s1, s2, sx, actionspace = initialise()
    Q = qtable(s1, s2, actionspace)

    actionspace1 = actionval(actionspace)
    
    for i in 1:episodes    
        epsilon = 1
        obs, done = reset1(sx)
        episode_reward = 0
       for i in 1:iter
            if epsilon<rand()
                action = maxaction(obs, Q, actionspace)
            else
                action = randomaction(actionspace)
            end
            
            obs_n, reward, done = move(actionspace1[action], obs)            
            episode_reward = episode_reward + reward
            action_f = maxaction(obs_n, Q, actionspace)

            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] 
                                                - Q[obs, action])) 
            
            obs = obs_n
            
            if epsilon>0.1
                epsilon = epsilon*0.99
            else
                continue
            end         
        end
        if i%10 == 0
                score = []            
                for pxc in 0:2:42
                    trk = 0
                    for l in s1
                        for n in s2
                            currentstate2 = (l,n)
                            for j in 1:pxc                      
                                currentaction2= maxaction(currentstate2, Q, actionspace)
                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)
                                currentstate2 = cnewstate2                                     
                            end
                            if currentstate2 == (0,0)
                                    trk += 1
                                
                            end

                        end
                    end
                    push!(score,(trk/(length(s1)*length(s2)))*100)
                    
                end 
                push!(score_cumulative,score)
            end
        push!(totalr,episode_reward)
        println("Episode Reward:", episode_reward)
    end
    return Q,totalr, score_cumulative
end

Q, totalr, score_cumulative = main()

"""
# Data visualisation stuff

function encode_act(act,actionspace)
    for l = 1:length(actionspace)
        if act == actionspace[l]
            return l
        end
    end
end

bestQ = Matrix{Int8}(undef,size(s1)[1],size(s2)[1])
for i = -20:1:20
    for j = 20:-1:-20
        bestQ[abs(j-21),i+21] = encode_act(maxaction((i,j),Q,actionspace),actionspace)      
    end
end

CSV.write("Qsym.csv", Tables.table(bestQ), writeheader=false)

heatmap([i for i = -20:1:20],[i for i = -20:1:20],bestQ)

hm = heatmap([i for i = -20:1:20],
    [i for i = -20:1:20], bestQ,
    c=cgrad([:blue, :purple,:gray, :green, :red]),
    xlabel="X Axis", ylabel="Y Axis",
    title="Optimal Action Choice Per State",
    #color=:thermal, 
    #c = cgrad(:thermal) ,
    colorbar_title="Action Choices",
    )

function op()
        m = 0
        n = 0
        for i in s1
            for j in s2
                currentstate2 = (i,j)
                done = false
                t = 0
                while !done
                    currentaction2 = maxaction(currentstate2, Q, actionspace)
                    cnewstate2,_,done = move(actionspace1[currentaction2], currentstate2)
                    currentstate2 = cnewstate2
                    t+=1
                end
                if t%2 == 0
                    m+=1
                else
                    n+=1
                end
            end
    end
    return m,n
end

p1,p2=op()

anim = @animate for i in 1:1000
    plot([i for i = 0:2:42],score_cumulative[i],legend=:bottomright,title = string("Policy Comparison (Grid World) at Episode ",(i*10)) , label = ["No_sym"],xlim=(30,42), ylim=(0, 100))
    plot!([i for i = 0:2:42],score_cumulative_ysym[i],legend=:bottomright, label = ["Y_sym"],xlim=(0,42), ylim=(0, 100))
    plot!([i for i = 0:2:42],score_cumulative_xsym[i],legend=:bottomright, label = ["X_sym"],xlim=(0,42), ylim=(0, 100))
    plot!([i for i = 0:2:42],score_cumulative_xysym[i],legend=:bottomright, label = ["XY_sym"],xlim=(0,42), ylim=(0, 100))
    #plot!(bs[i],legend=:bottomright, label = ["Best Case"])
    xlabel!("Number of Steps")     
    ylabel!("Percentage Success")
end

gif(anim, fps=15)

"""

function ysymmetric_state(state)
    return (-state[1],state[2])
end

function ysymmetric_action(action)
    if action == "U" || action == "D" || action == "N"
        return action
        elseif action == "L"
            return "R"
            else return "L"
        end
    end

# Exploit only y-axis symmetry

function ysym_main()
    alph = 0.3
    gamm = 0.9
    episodes = 2
    iter = 300
    
    totalr = []
    score_cumulative = []
    s1, s2, sx, actionspace = initialise()
    Q = qtable(s1, s2, actionspace)

    actionspace1 = actionval(actionspace)
    for i in 1:episodes    
        #j=0
        epsilon = 1
        obs, done = reset1(sx)
        episode_reward = 0
        println("Printing iteration:", i)
        for i in 1:iter
            if epsilon<rand()
                action = maxaction(obs, Q, actionspace)
            else
                action = randomaction(actionspace)
            end
            
            obs_n, reward, done = move(actionspace1[action], obs)            
            episode_reward = episode_reward + reward
            action_f = maxaction(obs_n, Q, actionspace)

            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] 
                                                - Q[obs, action])) 
            
            ysym_obs = ysymmetric_state(obs)
            ysym_action = ysymmetric_action(action)
            
            Q[ysym_obs, ysym_action] = deepcopy(Q[obs, action])
                       
            obs = obs_n
            
            if epsilon>0.1
                epsilon = epsilon*0.99
            else
                continue
            end  
            
        end
        if i%10 == 0
                score = []            
                for pxc in 0:2:42
                    trk = 0
                    for l in s1
                        for n in s2
                            currentstate2 = (l,n)
                            for j in 1:pxc                      
                                currentaction2= maxaction(currentstate2, Q, actionspace)
                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)
                                currentstate2 = cnewstate2
                                
                            end
                            if currentstate2 == (0,0)
                                    trk += 1
                                
                                end

                        end
                    end
                    push!(score,(trk/(length(s1)*length(s2)))*100)
                    
                end 
                push!(score_cumulative,score)
            end
        push!(totalr,episode_reward)
        println("Episode Reward:", episode_reward)
    end
    return Q,totalr, score_cumulative
end

Q_ysym, totalr_ysym, score_cumulative_ysym = ysym_main()

function xsymmetric_state(state)
    return (state[1],-state[2])
end

function xsymmetric_action(action)
    if action == "L" || action == "R" || action == "N"
        return action
        elseif action == "D"
            return "U"
        else return "D"
        end
    end

# Exploit only x-axis symmetry

function xsym_main()
    alph = 0.3
    gamm = 0.6
    episodes = 10000
    iter = 300
    
    totalr = []
    score_cumulative = []
    s1, s2, sx, actionspace = initialise()
    Q = qtable(s1, s2, actionspace)

    actionspace1 = actionval(actionspace)
    for i in 1:episodes    
        #j=0
        epsilon = 1
        obs, done = reset1(sx)
        episode_reward = 0
        println("Printing iteration:", i)
        for i in 1:iter
            if epsilon<rand()
                action = maxaction(obs, Q, actionspace)
            else
                action = randomaction(actionspace)
            end
            
            obs_n, reward, done = move(actionspace1[action], obs)
                #println(actionspace1[action])
            #println(obs_n)
            
            episode_reward = episode_reward + reward
            action_f = maxaction(obs_n, Q, actionspace)

            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] 
                                                - Q[obs, action])) 
            
            xsym_obs = xsymmetric_state(obs)
            xsym_action = xsymmetric_action(action)
            
            Q[xsym_obs, xsym_action] = Q[obs, action]
        
            obs = obs_n
            
            if epsilon>0.1
                epsilon = epsilon*0.99
            else
                continue
            end  
        end
        if i%10 == 0
                score = []            
                for pxc in 0:2:42
                    trk = 0
                    for l in s1
                        for n in s2
                            currentstate2 = (l,n)
                            for j in 1:pxc                      
                                currentaction2= maxaction(currentstate2, Q, actionspace)
                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)
                                currentstate2 = cnewstate2
                                
                            end
                            if currentstate2 == (0,0)
                                    trk += 1
                                    
                                end

                        end
                    end
                    push!(score,(trk/(length(s1)*length(s2)))*100)
                    
                end 
                push!(score_cumulative,score)
            end
        push!(totalr,episode_reward)
        println("Episode Reward:", episode_reward)
    end
    return Q,totalr, score_cumulative
end

Q_xsym, totalr_xsym, score_cumulative_xsym = xsym_main()

function xysymmetric_state(state)
    return (-state[1],-state[2])
end

function xysymmetric_action(action)
    if action == "N"
        return action
        elseif action == "L"
            return "R"
        elseif action== "R"
            return "L"
        elseif action == "D"
            return "U"
        else return "D"
        end
    end

# Exploit both x and y axis symmetries

function xysym_main()
    alph = 0.3
    gamm = 0.6
    episodes = 10000
    iter = 300
    
    totalr = []
    score_cumulative = []
    s1, s2, sx, actionspace = initialise()
    Q = qtable(s1, s2, actionspace)

    actionspace1 = actionval(actionspace)
    for i in 1:episodes    
        #j=0
        epsilon = 1
        obs, done = reset1(sx)
        episode_reward = 0
        println("Printing iteration:", i)
        for i in 1:iter
            if epsilon<rand()
                action = maxaction(obs, Q, actionspace)
            else
                action = randomaction(actionspace)
            end
            
            obs_n, reward, done = move(actionspace1[action], obs)
                #println(actionspace1[action])
            #println(obs_n)
            
            episode_reward = episode_reward + reward
            action_f = maxaction(obs_n, Q, actionspace)

            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] 
                                                - Q[obs, action])) 
            
            xsym_obs = xsymmetric_state(obs)
            xsym_action = xsymmetric_action(action)
            ysym_obs = ysymmetric_state(obs)
            ysym_action = ysymmetric_action(action)
            xysym_obs = xysymmetric_state(obs)
            xysym_action = xysymmetric_action(action)
            
            
            Q[xsym_obs, xsym_action] = Q[obs, action]             
            Q[ysym_obs, ysym_action] = Q[obs, action]
            Q[xysym_obs, xysym_action] = Q[obs, action] 
            
        
            obs = obs_n
            
            if epsilon>0.1
                epsilon = epsilon*0.99
            else
                continue
            end  
        end
        if i%10 == 0
                score = []            
                for pxc in 0:2:42
                    trk = 0
                    for l in s1
                        for n in s2
                            currentstate2 = (l,n)
                            for j in 1:pxc                      
                                currentaction2= maxaction(currentstate2, Q, actionspace)
                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)
                                currentstate2 = cnewstate2
                                
                            end
                            if currentstate2 == (0,0)
                                    trk += 1
                                    
                                end

                        end
                    end
                    push!(score,(trk/(length(s1)*length(s2)))*100)
                    
                end 
                push!(score_cumulative,score)
            end
        push!(totalr,episode_reward)
        println("Episode Reward:", episode_reward)
    end
    return Q,totalr, score_cumulative
end

Q_xysym, totalr_xysym, score_cumulative_xysym = xysym_main()


