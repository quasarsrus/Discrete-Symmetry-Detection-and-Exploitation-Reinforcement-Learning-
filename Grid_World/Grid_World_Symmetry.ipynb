{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944267c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using HDF5\n",
    "using Plots\n",
    "using Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dde567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise state space\n",
    "\n",
    "function initialise()\n",
    "    s1 = Vector{Float64}()\n",
    "    s2 = Vector{Float64}()\n",
    "    s1 = range(-20,stop=20, length = 41);\n",
    "    s2 = range(-20,stop=20, length = 41);\n",
    "    sx = hcat(s1,s2)\n",
    "\n",
    "    actionspace = [\"U\", \"D\", \"R\", \"L\", \"N\"]\n",
    "    \n",
    "    return s1, s2, sx, actionspace\n",
    "end\n",
    "\n",
    "# Initialise action space\n",
    "\n",
    "function actionval(actionspace)\n",
    "    actionspace1 = Dict()\n",
    "    for i in 1:length(actionspace)\n",
    "        if actionspace[i] == \"U\" || actionspace[i] == \"D\"\n",
    "            actionspace1[actionspace[i]] = (0,-1*(-1)^i)\n",
    "        elseif  actionspace[i] == \"N\"\n",
    "            actionspace1[actionspace[i]] = (0,0)               \n",
    "        else\n",
    "            actionspace1[actionspace[i]] = (-1*(-1)^i,0)\n",
    "        end\n",
    "    end\n",
    "    return actionspace1\n",
    "end\n",
    "\n",
    "# Define the dynamics and reward function\n",
    "\n",
    "function move(action, state)\n",
    "    \n",
    "    reward = 0\n",
    "    \n",
    "    statex = state[1] + action[1]\n",
    "    statey = state[2] + action[2]\n",
    "    if (statex,statey) == (0,0)\n",
    "        reward = 100\n",
    "        done = true\n",
    "        return (statex,statey), reward, done\n",
    "    \n",
    "    elseif statex > 20.0 || statex < -20.0 || statey > 20.0 || statey < -20.0\n",
    "        reward = -50\n",
    "        done = false\n",
    "        return (state[1],state[2]), reward, done\n",
    "    \n",
    "    else\n",
    "        reward = -10\n",
    "        done = false\n",
    "        return (statex,statey), reward, done  \n",
    "    end\n",
    "end\n",
    "\n",
    "# Reset the states\n",
    "\n",
    "function reset1(sx)\n",
    "    return (sx[rand(1:length(sx[:,1])),1],sx[rand(1:length(sx[:,2])),2]), false\n",
    "end\n",
    "\n",
    "# Choose random action\n",
    "            \n",
    "function randomaction(actionspace)\n",
    "    return actionspace[rand(1:length(actionspace))]\n",
    "end\n",
    "\n",
    "# Choose the action with the maximum Q-value for a state\n",
    "                    \n",
    "function maxaction(state, Q, actionspace)\n",
    "    actval = [Q[state,i] for i in actionspace]\n",
    "    actind = argmax(actval)\n",
    "    return actionspace[actind]\n",
    "end\n",
    "\n",
    "# Initialise a Q-table\n",
    "    \n",
    "function qtable(s1, s2, actionspace)\n",
    "    \n",
    "    Q = Dict()\n",
    "    states= [] \n",
    "    for i in s1\n",
    "        for j in s2\n",
    "            push!(states,(i,j))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for i in states\n",
    "        for j in actionspace\n",
    "            Q[i,j] = 0 #(Int(rand(Int8)))^2\n",
    "        end\n",
    "    end\n",
    "    return Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function (no symmetry detection or exploitation)\n",
    "\n",
    "function main()\n",
    "    alph = 0.3\n",
    "    gamm = 0.9\n",
    "    episodes = 10\n",
    "    \n",
    "    iter = 300\n",
    "    \n",
    "    totalr = []\n",
    "    score_cumulative = []\n",
    "    s1, s2, sx, actionspace = initialise()\n",
    "    Q = qtable(s1, s2, actionspace)\n",
    "\n",
    "    actionspace1 = actionval(actionspace)\n",
    "    \n",
    "    for i in 1:episodes    \n",
    "        epsilon = 1\n",
    "        obs, done = reset1(sx)\n",
    "        episode_reward = 0\n",
    "       for i in 1:iter\n",
    "            if epsilon<rand()\n",
    "                action = maxaction(obs, Q, actionspace)\n",
    "            else\n",
    "                action = randomaction(actionspace)\n",
    "            end\n",
    "            \n",
    "            obs_n, reward, done = move(actionspace1[action], obs)            \n",
    "            episode_reward = episode_reward + reward\n",
    "            action_f = maxaction(obs_n, Q, actionspace)\n",
    "\n",
    "            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] \n",
    "                                                - Q[obs, action])) \n",
    "            \n",
    "            obs = obs_n\n",
    "            \n",
    "            if epsilon>0.1\n",
    "                epsilon = epsilon*0.99\n",
    "            else\n",
    "                continue\n",
    "            end         \n",
    "        end\n",
    "        if i%10 == 0\n",
    "                score = []            \n",
    "                for pxc in 0:2:42\n",
    "                    trk = 0\n",
    "                    for l in s1\n",
    "                        for n in s2\n",
    "                            currentstate2 = (l,n)\n",
    "                            for j in 1:pxc                      \n",
    "                                currentaction2= maxaction(currentstate2, Q, actionspace)\n",
    "                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)\n",
    "                                currentstate2 = cnewstate2                                     \n",
    "                            end\n",
    "                            if currentstate2 == (0,0)\n",
    "                                    trk += 1\n",
    "                                \n",
    "                            end\n",
    "\n",
    "                        end\n",
    "                    end\n",
    "                    push!(score,(trk/(length(s1)*length(s2)))*100)\n",
    "                    \n",
    "                end \n",
    "                push!(score_cumulative,score)\n",
    "            end\n",
    "        push!(totalr,episode_reward)\n",
    "        println(\"Episode Reward:\", episode_reward)\n",
    "    end\n",
    "    return Q,totalr, score_cumulative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6325b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, totalr, score_cumulative = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f470db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Data visualisation stuff\n",
    "\n",
    "function encode_act(act,actionspace)\n",
    "    for l = 1:length(actionspace)\n",
    "        if act == actionspace[l]\n",
    "            return l\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "bestQ = Matrix{Int8}(undef,size(s1)[1],size(s2)[1])\n",
    "for i = -20:1:20\n",
    "    for j = 20:-1:-20\n",
    "        bestQ[abs(j-21),i+21] = encode_act(maxaction((i,j),Q,actionspace),actionspace)      \n",
    "    end\n",
    "end\n",
    "\n",
    "CSV.write(\"Qsym.csv\", Tables.table(bestQ), writeheader=false)\n",
    "\n",
    "heatmap([i for i = -20:1:20],[i for i = -20:1:20],bestQ)\n",
    "\n",
    "hm = heatmap([i for i = -20:1:20],\n",
    "    [i for i = -20:1:20], bestQ,\n",
    "    c=cgrad([:blue, :purple,:gray, :green, :red]),\n",
    "    xlabel=\"X Axis\", ylabel=\"Y Axis\",\n",
    "    title=\"Optimal Action Choice Per State\",\n",
    "    #color=:thermal, \n",
    "    #c = cgrad(:thermal) ,\n",
    "    colorbar_title=\"Action Choices\",\n",
    "    )\n",
    "\n",
    "function op()\n",
    "        m = 0\n",
    "        n = 0\n",
    "        for i in s1\n",
    "            for j in s2\n",
    "                currentstate2 = (i,j)\n",
    "                done = false\n",
    "                t = 0\n",
    "                while !done\n",
    "                    currentaction2 = maxaction(currentstate2, Q, actionspace)\n",
    "                    cnewstate2,_,done = move(actionspace1[currentaction2], currentstate2)\n",
    "                    currentstate2 = cnewstate2\n",
    "                    t+=1\n",
    "                end\n",
    "                if t%2 == 0\n",
    "                    m+=1\n",
    "                else\n",
    "                    n+=1\n",
    "                end\n",
    "            end\n",
    "    end\n",
    "    return m,n\n",
    "end\n",
    "\n",
    "p1,p2=op()\n",
    "\n",
    "anim = @animate for i in 1:1000\n",
    "    plot([i for i = 0:2:42],score_cumulative[i],legend=:bottomright,title = string(\"Policy Comparison (Grid World) at Episode \",(i*10)) , label = [\"No_sym\"],xlim=(30,42), ylim=(0, 100))\n",
    "    plot!([i for i = 0:2:42],score_cumulative_ysym[i],legend=:bottomright, label = [\"Y_sym\"],xlim=(0,42), ylim=(0, 100))\n",
    "    plot!([i for i = 0:2:42],score_cumulative_xsym[i],legend=:bottomright, label = [\"X_sym\"],xlim=(0,42), ylim=(0, 100))\n",
    "    plot!([i for i = 0:2:42],score_cumulative_xysym[i],legend=:bottomright, label = [\"XY_sym\"],xlim=(0,42), ylim=(0, 100))\n",
    "    #plot!(bs[i],legend=:bottomright, label = [\"Best Case\"])\n",
    "    xlabel!(\"Number of Steps\")     \n",
    "    ylabel!(\"Percentage Success\")\n",
    "end\n",
    "\n",
    "gif(anim, fps=15)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ysymmetric_state(state)\n",
    "    return (-state[1],state[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0514410",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ysymmetric_action(action)\n",
    "    if action == \"U\" || action == \"D\" || action == \"N\"\n",
    "        return action\n",
    "        elseif action == \"L\"\n",
    "            return \"R\"\n",
    "            else return \"L\"\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploit only y-axis symmetry\n",
    "\n",
    "function ysym_main()\n",
    "    alph = 0.3\n",
    "    gamm = 0.9\n",
    "    episodes = 2\n",
    "    iter = 300\n",
    "    \n",
    "    totalr = []\n",
    "    score_cumulative = []\n",
    "    s1, s2, sx, actionspace = initialise()\n",
    "    Q = qtable(s1, s2, actionspace)\n",
    "\n",
    "    actionspace1 = actionval(actionspace)\n",
    "    for i in 1:episodes    \n",
    "        #j=0\n",
    "        epsilon = 1\n",
    "        obs, done = reset1(sx)\n",
    "        episode_reward = 0\n",
    "        println(\"Printing iteration:\", i)\n",
    "        for i in 1:iter\n",
    "            if epsilon<rand()\n",
    "                action = maxaction(obs, Q, actionspace)\n",
    "            else\n",
    "                action = randomaction(actionspace)\n",
    "            end\n",
    "            \n",
    "            obs_n, reward, done = move(actionspace1[action], obs)            \n",
    "            episode_reward = episode_reward + reward\n",
    "            action_f = maxaction(obs_n, Q, actionspace)\n",
    "\n",
    "            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] \n",
    "                                                - Q[obs, action])) \n",
    "            \n",
    "            ysym_obs = ysymmetric_state(obs)\n",
    "            ysym_action = ysymmetric_action(action)\n",
    "            \n",
    "            Q[ysym_obs, ysym_action] = deepcopy(Q[obs, action])\n",
    "                       \n",
    "            obs = obs_n\n",
    "            \n",
    "            if epsilon>0.1\n",
    "                epsilon = epsilon*0.99\n",
    "            else\n",
    "                continue\n",
    "            end  \n",
    "            \n",
    "        end\n",
    "        if i%10 == 0\n",
    "                score = []            \n",
    "                for pxc in 0:2:42\n",
    "                    trk = 0\n",
    "                    for l in s1\n",
    "                        for n in s2\n",
    "                            currentstate2 = (l,n)\n",
    "                            for j in 1:pxc                      \n",
    "                                currentaction2= maxaction(currentstate2, Q, actionspace)\n",
    "                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)\n",
    "                                currentstate2 = cnewstate2\n",
    "                                \n",
    "                            end\n",
    "                            if currentstate2 == (0,0)\n",
    "                                    trk += 1\n",
    "                                \n",
    "                                end\n",
    "\n",
    "                        end\n",
    "                    end\n",
    "                    push!(score,(trk/(length(s1)*length(s2)))*100)\n",
    "                    \n",
    "                end \n",
    "                push!(score_cumulative,score)\n",
    "            end\n",
    "        push!(totalr,episode_reward)\n",
    "        println(\"Episode Reward:\", episode_reward)\n",
    "    end\n",
    "    return Q,totalr, score_cumulative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc43bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_ysym, totalr_ysym, score_cumulative_ysym = ysym_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b904b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function xsymmetric_state(state)\n",
    "    return (state[1],-state[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function xsymmetric_action(action)\n",
    "    if action == \"L\" || action == \"R\" || action == \"N\"\n",
    "        return action\n",
    "        elseif action == \"D\"\n",
    "            return \"U\"\n",
    "        else return \"D\"\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploit only x-axis symmetry\n",
    "\n",
    "function xsym_main()\n",
    "    alph = 0.3\n",
    "    gamm = 0.6\n",
    "    episodes = 10000\n",
    "    iter = 300\n",
    "    \n",
    "    totalr = []\n",
    "    score_cumulative = []\n",
    "    s1, s2, sx, actionspace = initialise()\n",
    "    Q = qtable(s1, s2, actionspace)\n",
    "\n",
    "    actionspace1 = actionval(actionspace)\n",
    "    for i in 1:episodes    \n",
    "        #j=0\n",
    "        epsilon = 1\n",
    "        obs, done = reset1(sx)\n",
    "        episode_reward = 0\n",
    "        println(\"Printing iteration:\", i)\n",
    "        for i in 1:iter\n",
    "            if epsilon<rand()\n",
    "                action = maxaction(obs, Q, actionspace)\n",
    "            else\n",
    "                action = randomaction(actionspace)\n",
    "            end\n",
    "            \n",
    "            obs_n, reward, done = move(actionspace1[action], obs)\n",
    "                #println(actionspace1[action])\n",
    "            #println(obs_n)\n",
    "            \n",
    "            episode_reward = episode_reward + reward\n",
    "            action_f = maxaction(obs_n, Q, actionspace)\n",
    "\n",
    "            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] \n",
    "                                                - Q[obs, action])) \n",
    "            \n",
    "            xsym_obs = xsymmetric_state(obs)\n",
    "            xsym_action = xsymmetric_action(action)\n",
    "            \n",
    "            Q[xsym_obs, xsym_action] = Q[obs, action]\n",
    "        \n",
    "            obs = obs_n\n",
    "            \n",
    "            if epsilon>0.1\n",
    "                epsilon = epsilon*0.99\n",
    "            else\n",
    "                continue\n",
    "            end  \n",
    "        end\n",
    "        if i%10 == 0\n",
    "                score = []            \n",
    "                for pxc in 0:2:42\n",
    "                    trk = 0\n",
    "                    for l in s1\n",
    "                        for n in s2\n",
    "                            currentstate2 = (l,n)\n",
    "                            for j in 1:pxc                      \n",
    "                                currentaction2= maxaction(currentstate2, Q, actionspace)\n",
    "                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)\n",
    "                                currentstate2 = cnewstate2\n",
    "                                \n",
    "                            end\n",
    "                            if currentstate2 == (0,0)\n",
    "                                    trk += 1\n",
    "                                    \n",
    "                                end\n",
    "\n",
    "                        end\n",
    "                    end\n",
    "                    push!(score,(trk/(length(s1)*length(s2)))*100)\n",
    "                    \n",
    "                end \n",
    "                push!(score_cumulative,score)\n",
    "            end\n",
    "        push!(totalr,episode_reward)\n",
    "        println(\"Episode Reward:\", episode_reward)\n",
    "    end\n",
    "    return Q,totalr, score_cumulative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ec4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_xsym, totalr_xsym, score_cumulative_xsym = xsym_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e5633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function xysymmetric_state(state)\n",
    "    return (-state[1],-state[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837dce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "function xysymmetric_action(action)\n",
    "    if action == \"N\"\n",
    "        return action\n",
    "        elseif action == \"L\"\n",
    "            return \"R\"\n",
    "        elseif action== \"R\"\n",
    "            return \"L\"\n",
    "        elseif action == \"D\"\n",
    "            return \"U\"\n",
    "        else return \"D\"\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb04fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploit both x and y axis symmetries\n",
    "\n",
    "function xysym_main()\n",
    "    alph = 0.3\n",
    "    gamm = 0.6\n",
    "    episodes = 10000\n",
    "    iter = 300\n",
    "    \n",
    "    totalr = []\n",
    "    score_cumulative = []\n",
    "    s1, s2, sx, actionspace = initialise()\n",
    "    Q = qtable(s1, s2, actionspace)\n",
    "\n",
    "    actionspace1 = actionval(actionspace)\n",
    "    for i in 1:episodes    \n",
    "        #j=0\n",
    "        epsilon = 1\n",
    "        obs, done = reset1(sx)\n",
    "        episode_reward = 0\n",
    "        println(\"Printing iteration:\", i)\n",
    "        for i in 1:iter\n",
    "            if epsilon<rand()\n",
    "                action = maxaction(obs, Q, actionspace)\n",
    "            else\n",
    "                action = randomaction(actionspace)\n",
    "            end\n",
    "            \n",
    "            obs_n, reward, done = move(actionspace1[action], obs)\n",
    "                #println(actionspace1[action])\n",
    "            #println(obs_n)\n",
    "            \n",
    "            episode_reward = episode_reward + reward\n",
    "            action_f = maxaction(obs_n, Q, actionspace)\n",
    "\n",
    "            Q[obs, action] = (Q[obs, action] + alph*(reward + gamm*Q[obs_n,action_f] \n",
    "                                                - Q[obs, action])) \n",
    "            \n",
    "            xsym_obs = xsymmetric_state(obs)\n",
    "            xsym_action = xsymmetric_action(action)\n",
    "            ysym_obs = ysymmetric_state(obs)\n",
    "            ysym_action = ysymmetric_action(action)\n",
    "            xysym_obs = xysymmetric_state(obs)\n",
    "            xysym_action = xysymmetric_action(action)\n",
    "            \n",
    "            \n",
    "            Q[xsym_obs, xsym_action] = Q[obs, action]             \n",
    "            Q[ysym_obs, ysym_action] = Q[obs, action]\n",
    "            Q[xysym_obs, xysym_action] = Q[obs, action] \n",
    "            \n",
    "        \n",
    "            obs = obs_n\n",
    "            \n",
    "            if epsilon>0.1\n",
    "                epsilon = epsilon*0.99\n",
    "            else\n",
    "                continue\n",
    "            end  \n",
    "        end\n",
    "        if i%10 == 0\n",
    "                score = []            \n",
    "                for pxc in 0:2:42\n",
    "                    trk = 0\n",
    "                    for l in s1\n",
    "                        for n in s2\n",
    "                            currentstate2 = (l,n)\n",
    "                            for j in 1:pxc                      \n",
    "                                currentaction2= maxaction(currentstate2, Q, actionspace)\n",
    "                                cnewstate2,_,_ = move(actionspace1[currentaction2], currentstate2)\n",
    "                                currentstate2 = cnewstate2\n",
    "                                \n",
    "                            end\n",
    "                            if currentstate2 == (0,0)\n",
    "                                    trk += 1\n",
    "                                    \n",
    "                                end\n",
    "\n",
    "                        end\n",
    "                    end\n",
    "                    push!(score,(trk/(length(s1)*length(s2)))*100)\n",
    "                    \n",
    "                end \n",
    "                push!(score_cumulative,score)\n",
    "            end\n",
    "        push!(totalr,episode_reward)\n",
    "        println(\"Episode Reward:\", episode_reward)\n",
    "    end\n",
    "    return Q,totalr, score_cumulative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc22bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_xysym, totalr_xysym, score_cumulative_xysym = xysym_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192758a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
